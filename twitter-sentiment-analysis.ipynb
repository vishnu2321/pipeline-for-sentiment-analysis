{"cells":[{"cell_type":"code","source":["#running for first time on a new cluster\n# !pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799a6c74-2a8d-4bcc-a35f-db59475aff68"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# import nltk\n# nltk.download(\"stopwords\")\n# nltk.download('wordnet')\n# nltk.download('omw-1.4')   #patch for error on wordnet lemmatizer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81625f44-242f-46e8-891f-54f631125556"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# kaggle dataset --- https://www.kaggle.com/datasets/kazanova/sentiment140\n\n# File location and type\nfile_location = \"/FileStore/tables/data.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf = df.repartition(4)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Loading Data and Repartitioning dataframe","showTitle":true,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# dropping timestamp, user_ids and usernames\ndf = df.drop(\"_c1\",\"_c2\",\"_c3\",\"_c4\").toDF(\"label\",\"tweet\")\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e9ec976-bd42-4ecf-99eb-060e9cad5781"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Schema of dataframe","showTitle":true,"inputWidgets":{},"nuid":"5ef9c82d-b722-4e45-901e-480e8077b8b0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"data_csv\"\n\ndf.createOrReplaceTempView(temp_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd82bb99-1479-4d5c-be10-8c36df0f1d44"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `data_csv`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f66379-6f7f-42ec-8e82-d0e0926a1721"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nselect count(\"label\") as label from `data_csv` group by label"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Balanced dataset ","showTitle":true,"inputWidgets":{},"nuid":"fd3c5351-eb56-45d1-937c-ab63ab9871f8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\"\"\"\n    -ve tweet is intially given 4 in label column, it is replaced with 1 for simpler understanding and esay evaluation\n\"\"\"\n\nfrom pyspark.sql.functions import * \ndf = df.withColumn(\"label\",when(df[\"label\"]==\"4\",\"1\").otherwise(\"0\"))\ndf = df.withColumn(\"label\",df.label.cast(\"integer\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c85b935d-c87a-4766-bc15-3d2b7e77331d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# CUSTOM TRANSFORMER\nimport nltk\nimport string,re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk import WordNetLemmatizer\nfrom pyspark.ml.feature import HashingTF, IDF\nfrom pyspark.ml import Transformer\n\nclass PreprocessingTweetTransformer(Transformer):\n    \"\"\"\n        The tweet is preprocessed by several methods which include converting to lower-case, removing stopwords, removing puntuations,\n        removing repeating characters, removing URLs syntax, removal of numbers, seperating to tokens, stemming, lemmatization and\n        TFIDF Vectorization.\n    \"\"\"\n    def __init__(self):\n        super(PreprocessingTweetTransformer,self).__init__()\n        self.STOPWORDS = stopwords.words('english')\n        self.english_punctuations = string.punctuation\n        self.punctuations_list = self.english_punctuations\n        self.tokenizer = RegexpTokenizer(pattern = \"\\s+\",gaps=True)\n        self.st = nltk.PorterStemmer()\n        self.lm = WordNetLemmatizer()\n        \n    def _transform(self, df:DataFrame) -> DataFrame:\n        \"\"\" \n            Input: DataFrame\n            Output: DataFrame\n            Description: Tranforms the given dataframe to required dataframe using several methods mentioned below.\n        \"\"\"\n    \n        lowercased_df = df.rdd.map(lambda row:[row.label, self.to_lower(row.tweet)]).toDF()\n        removed_stopwords_df = lowercased_df.rdd.map(lambda row:[row._1, self.removing_stopwords(row._2)]).toDF()\n        cleaned_punctuations_df = removed_stopwords_df.rdd.map(lambda row:[row._1, self.cleaning_punctuations(row._2)]).toDF()\n        cleaned_repeating_char_df = cleaned_punctuations_df.rdd.map(lambda row:[row._1, self.cleaning_repeating_char(row._2)]).toDF()\n        cleaned_URL_df = cleaned_repeating_char_df.rdd.map(lambda row:[row._1, self.cleaning_URLs(row._2)]).toDF()\n        cleaned_numbers_df = cleaned_URL_df.rdd.map(lambda row:[row._1, self.cleaning_numbers(row._2)]).toDF()\n        tokenized_df = cleaned_numbers_df.rdd.map(lambda row:[row._1, self.tokenize(row._2)]).toDF()\n        stemmed_df = tokenized_df.rdd.map(lambda row:[row._1, self.stemming(row._2)]).toDF()\n        lemmatized_df = stemmed_df.rdd.map(lambda row:[row._1, self.lemmatizer(row._2)]).toDF()\n        res_df = lemmatized_df.toDF(\"label\",\"tweet\")\n        return res_df\n    \n    def to_lower(self, text:String) -> String:\n        \"\"\" \n            Input: String\n            Output: String\n            Description: Converts the given string to lowered case string.\n        \"\"\"\n        return text.lower()\n    \n    def removing_stopwords(self, text:String) -> String:\n        \"\"\" \n            Input: String\n            Output: String\n            Description: Removes stop words such as at, a, an, the etc which donot add meaning to the given text.\n        \"\"\"\n        return \" \".join([word for word in str(text).split() if word not in self.STOPWORDS])\n    \n    def cleaning_punctuations(self,text:String) -> String:\n        \"\"\" \n            Input: String\n            Output: String\n            Description: Removes the punctuations if present in the given text\n        \"\"\"\n        translator = str.maketrans('', '', self.punctuations_list)\n        return text.translate(translator)\n    \n    def cleaning_repeating_char(self, text:String) -> String:\n        \"\"\" \n            Input: String\n            Output: String\n            Description: Removes repeating characters, such as aaaah is converted to ah.\n        \"\"\"\n        return re.sub(r'(.)1+', r'1', text)\n\n    def cleaning_URLs(self, text:String) -> String:\n        \"\"\" \n            Input: String\n            Output: String\n            Description: Removes the https:// or http:// from the URL if present in the text.\n        \"\"\"\n        return re.sub('((www.[^s]+)|(https?://[^s]+)|(http?://[^s]+))',' ',text)\n    \n    def cleaning_numbers(self,text:String) -> String:\n        \"\"\" \n            Input: String\n            Output: String\n            Description: Removes the numerals if present in the text.\n        \"\"\"\n        return re.sub('[0-9]+', '', text)\n    \n    def tokenize(self, text:String) -> List:\n        \"\"\" \n            Input: String\n            Output: List\n            Description: Splits the text into smaller tokens.For example, \"Hello World\" is converted to [\"Hello\",\"World\"]\n        \"\"\"\n        return self.tokenizer.tokenize(text)\n    \n    def stemming(self, data:List) -> List:\n        \"\"\" \n            Input: List\n            Output: List\n            Description: Converts every word to its root/base word if for a given word, it exists.\n        \"\"\"\n        text = [self.st.stem(word) for word in data]\n        return data\n    \n    def lemmatizer(self, data:List) -> List:\n        \"\"\" \n            Input: List\n            Output: List\n            Description: Removes the punctuations if present in the given text\n        \"\"\"\n        text = [self.lm.lemmatize(word) for word in data]\n        return text"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Custom Transformer with all preprocessing on the dataframe for pipeline","showTitle":true,"inputWidgets":{},"nuid":"d9aa10c1-d1c9-42c9-9f4c-e945353f76fa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stages = []\n\ntransformer = PreprocessingTweetTransformer()\nstages+=[transformer]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stages for the pipeline","showTitle":true,"inputWidgets":{},"nuid":"9ddf4d68-bb4f-424b-a437-909761b6ad8d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF\n\nhashingTF = HashingTF(inputCol = \"tweet\",outputCol = \"hashedTF_tweet\",)\nstages+=[hashingTF]\nidf = IDF(inputCol = \"hashedTF_tweet\",outputCol = \"idf_features\")\nstages+=[idf]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"TF-IDF Vectorizer using hashingTF and IDF","showTitle":true,"inputWidgets":{},"nuid":"2f0b0d77-2559-430c-872d-2f70e3778e71"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npreprocessing_pipeline = Pipeline(stages=stages)\nmodel = preprocessing_pipeline.fit(df)\ndf_updated = model.transform(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Fitting and transforming dataframe using pipeline","showTitle":true,"inputWidgets":{},"nuid":"fb34129b-ef99-482f-86f7-2ec6cea4fe28"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train_df, test_df = df_updated.randomSplit([0.7,0.3],24)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Splitting dataframe for training and testing","showTitle":true,"inputWidgets":{},"nuid":"c0afdf08-2e9e-44ad-bc61-b90058bbc78d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol = 'idf_features', labelCol = 'label', maxIter=10000)\nlrModel = lr.fit(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Logistic Regression Model","showTitle":true,"inputWidgets":{},"nuid":"bac3480b-3e5a-46bc-92d8-0c41a159e96c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba6540aa-d6c8-4535-bc23-1f4dafd2aaac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["trainingSummary = lrModel.summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48c8ebe4-2f04-4e73-b318-cf5e437f652e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["predictions = lrModel.transform(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Predicting test data","showTitle":true,"inputWidgets":{},"nuid":"61b80e69-02b4-42d5-8965-e9b51fdf5c68"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["predictions.select(\"label\",\"prediction\",\"probability\").display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"965b9834-82d0-400b-a335-a926f8680ce5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', evaluator.evaluate(predictions))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e7ff40b-09ce-4166-b885-cc51f376d7f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(lrModel.summary.accuracy)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Accuracy","showTitle":true,"inputWidgets":{},"nuid":"37466ecc-b28a-4fc8-9076-c7415bcfcce2"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"twitter-sentiment-analysis","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2287481793680727}},"nbformat":4,"nbformat_minor":0}
